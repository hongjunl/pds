{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLDA Topic Modeling and Data Visualization\n",
    "\n",
    "This project is a pratical scenario for us to implement the knowledge of this course.\n",
    "In this project, we will first collect data with a crawler and then use LLDA to get the topic of the data collected by us and then using data visualization tools to display the result of our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Using Scrapy to Get Data from NTRS\n",
    "\n",
    "Scrapy is a very good web crawling framework. We have finished the crawler and have successfully almost 6000 papers from NTRS(NASA Technical Reports Server). Within this framework, we can easily handle request and add dely of each request to make sure that we will not put so much pressure on the server.\n",
    "\n",
    "First create a new scrapy project:\n",
    "```bash\n",
    "scrapy startproject papercrawler\n",
    "```\n",
    "We will get the following project:\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── Resources\n",
    "├── items.json\n",
    "├── paper_crawler\n",
    "│   ├── __init__.py\n",
    "│   ├── __init__.pyc\n",
    "│   ├── items.py\n",
    "│   ├── items.pyc\n",
    "│   ├── pipelines.py\n",
    "│   ├── pipelines.pyc\n",
    "│   ├── settings.py\n",
    "│   ├── settings.pyc\n",
    "│   └── spiders\n",
    "│       ├── __init__.py\n",
    "│       ├── __init__.pyc\n",
    "│       ├── paper_spider.py\n",
    "│       └── paper_spider.pyc\n",
    "├── scrapy.cfg\n",
    "└── urls\n",
    "```\n",
    "We need to configure the following parts to make our crawler work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Define Items\n",
    "\n",
    "To use scrapy to crawl papers from NASA, we need to first define items, which is the data structure we will to get from website. Define as following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class PaperCrawlerItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field();\n",
    "    ntrs_full_text = scrapy.Field();\n",
    "    author_and_affiliation = scrapy.Field();\n",
    "    abstract = scrapy.Field();\n",
    "    publication_date = scrapy.Field();\n",
    "    document_id = scrapy.Field();\n",
    "    subject_category = scrapy.Field();\n",
    "    patent_number = scrapy.Field();\n",
    "    document_type = scrapy.Field();\n",
    "    meeting_information = scrapy.Field();\n",
    "    meeting_sponsor = scrapy.Field();\n",
    "    financial_sponsor = scrapy.Field();\n",
    "    organization_source = scrapy.Field();\n",
    "    description = scrapy.Field();\n",
    "    NASA_terms = scrapy.Field();\n",
    "    other_descriptors = scrapy.Field();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Defined entries for the urls in paper_spider.py:\n",
    "We need to input a pattern of a series of URLs so that the crawler can request the page iteratively, we need to analysis the start url from \"http://ntrs.nasa.gov/search.jsp?Ntx=mode%20matchall&Ntk=All&N=0&No=\" and use the pattern to request pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_requests(self):\n",
    "        for i in range(102231):\n",
    "            yield self.make_requests_from_url(\n",
    "                \"https://ntrs.nasa.gov/search.jsp?Ntx=mode%20matchall&Ntk=Title&N=0&Ntt=hurricane&No=\" + str(i * 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Define the function to parse the content of page and retrive information\n",
    "we carefully analyze the html DOM tree to find the values we need and used xpath to extract data we need. This is a very annoying task, because we need to handle every situation and sometimes the elements are not in the same format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_content(self, response):\n",
    "        title_list = response.xpath('//td[@id=\"recordtitle\"]//text()').extract()\n",
    "        title_string = \"\"\n",
    "        item = PaperCrawlerItem()\n",
    "        for word in title_list:\n",
    "            title_string += word\n",
    "        # print(str(title_string))\n",
    "        item['title'] = title_string\n",
    "\n",
    "        pdf_url = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"NTRS Full-Text:\")]/td/a[contains(text(),\"Click to View\")]/@href').extract()\n",
    "        item['ntrs_full_text'] = pdf_url\n",
    "\n",
    "        user_affilliation = [];\n",
    "        u_a = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Author and Affiliation:\")]/td/table/tr').extract()\n",
    "        if u_a != []:\n",
    "            for entry in u_a:\n",
    "                author = Selector(text=entry).xpath('//tr/td/text()').extract()\n",
    "                affilliation = Selector(text=entry).xpath('//tr/td/span/text()').extract()\n",
    "                temp = (author[0], affilliation[0].replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "                user_affilliation.append(temp)\n",
    "        else:\n",
    "            u_a = response.xpath(\n",
    "                '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Author and Affiliation:\")]/td/text()').extract()[1:]\n",
    "            # print(u_a)\n",
    "            for entry in u_a:\n",
    "                author = entry.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "                if author != \"\":\n",
    "                    temp = (author, \"\")\n",
    "                    user_affilliation.append(temp)\n",
    "        # print(user_affilliation)\n",
    "        item['author_and_affiliation'] = user_affilliation\n",
    "\n",
    "        abstract_list = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Abstract:\")]/td/descendant-or-self::*/text()').extract()\n",
    "        abstract = \"\"\n",
    "        for words in abstract_list:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Abstract:\" and words != \"\":\n",
    "                abstract += words\n",
    "        item[\"abstract\"] = abstract\n",
    "\n",
    "        publication_date_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Publication Date:\")]//text()').extract()\n",
    "        publication_date = \"\"\n",
    "        for words in publication_date_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Publication Date:\" and words != \"\":\n",
    "                publication_date = words;\n",
    "        item[\"publication_date\"] = publication_date\n",
    "\n",
    "        document_id_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Document ID:\")]/td/div[@id=\"docidDiv\"]/text()').extract()\n",
    "        document_id = \"\"\n",
    "        for words in document_id_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"\":\n",
    "                document_id = words;\n",
    "        item[\"document_id\"] = document_id\n",
    "\n",
    "        subject_category_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Subject Category:\")]//text()').extract()\n",
    "        subject_category = \"\"\n",
    "        for words in subject_category_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Subject Category:\" and words != \"\":\n",
    "                subject_category += words\n",
    "        item[\"subject_category\"] = subject_category\n",
    "\n",
    "        patent_number_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Report/Patent Number:\")]//text()').extract()\n",
    "        patent_number = \"\"\n",
    "        for words in patent_number_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Report/Patent Number:\" and words != \"\":\n",
    "                patent_number += words\n",
    "        item[\"patent_number\"] = patent_number\n",
    "\n",
    "        document_type_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Document Type:\")]//text()').extract()\n",
    "        document_type = \"\"\n",
    "        for words in document_type_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Document Type:\" and words != \"\":\n",
    "                document_type += words\n",
    "        item[\"document_type\"] = document_type\n",
    "\n",
    "        meeting_information_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Meeting Information:\")]//text()').extract()\n",
    "        meeting_information = \"\"\n",
    "        for words in meeting_information_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Meeting Information:\" and words != \"\":\n",
    "                meeting_information += words\n",
    "        item[\"meeting_information\"] = meeting_information\n",
    "\n",
    "        meeting_sponsor_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Meeting Sponsor:\")]//text()').extract()\n",
    "        meeting_sponsor = \"\"\n",
    "        for words in meeting_sponsor_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Meeting Sponsor:\" and words != \"\":\n",
    "                meeting_sponsor += words\n",
    "        item[\"meeting_sponsor\"] = meeting_sponsor\n",
    "\n",
    "        financial_sponsor_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Financial Sponsor:\")]//text()').extract()\n",
    "        financial_sponsor = \"\"\n",
    "        for words in financial_sponsor_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Financial Sponsor:\" and words != \"\":\n",
    "                financial_sponsor += words\n",
    "        item[\"financial_sponsor\"] = financial_sponsor\n",
    "\n",
    "        organization_source_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Organization Source:\")]//text()').extract()\n",
    "        organization_source = \"\"\n",
    "        for words in organization_source_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Organization Source:\" and words != \"\":\n",
    "                organization_source += words\n",
    "        item[\"organization_source\"] = organization_source\n",
    "\n",
    "        description_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Description:\")]//text()').extract()\n",
    "        description = \"\"\n",
    "        for words in description_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Description:\" and words != \"\":\n",
    "                description += words\n",
    "        item[\"description\"] = description\n",
    "\n",
    "        NASA_terms_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"NASA Terms:\")]//text()').extract()\n",
    "        NASA_terms = \"\"\n",
    "        for words in NASA_terms_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"NASA Terms:\" and words != \"\":\n",
    "                NASA_terms += words\n",
    "        item[\"NASA_terms\"] = NASA_terms\n",
    "\n",
    "        other_descriptors_raw = response.xpath(\n",
    "            '//table[@id=\"doctable\"]/tr[contains(td/text(),\"Other Descriptors:\")]//text()').extract()\n",
    "        other_descriptors = \"\"\n",
    "        for words in other_descriptors_raw:\n",
    "            words = words.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            if words != \"Other Descriptors:\" and words != \"\":\n",
    "                other_descriptors += words\n",
    "        item[\"other_descriptors\"] = other_descriptors\n",
    "\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Define Pipeline to store items to MongoDB\n",
    "After we extracted value from DOM tree and we create a new object item and then we used pipline to store in to MongoDB.\n",
    "Because at first, we assumed that every paper will have a document ID, but we found that, there are many papers without document ID. So, we created out own ID counter, since scrapy is a multi-thread framework, we need add lock to the counter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import threading\n",
    "\n",
    "class MongoPipeline(object):\n",
    "\n",
    "    collection_name = 'paper'\n",
    "    global counter\n",
    "    counter = 30000000000\n",
    "\n",
    "    global counter_lock\n",
    "    counter_lock = threading.Lock()\n",
    "\n",
    "    def __init__(self, mongo_uri, mongo_db):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            mongo_uri=crawler.settings.get('MONGO_URI'),\n",
    "            mongo_db=crawler.settings.get('MONGO_DATABASE', 'paper_crawler_all_new')\n",
    "        )\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if dict(item).get('document_id') == \"\":\n",
    "            global counter_lock\n",
    "            counter_lock.acquire()\n",
    "            try:\n",
    "                global counter\n",
    "                item['document_id'] = str(counter)\n",
    "                counter += 1\n",
    "            finally:\n",
    "                counter_lock.release()\n",
    "        self.db[self.collection_name].insert(dict(item))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Result of Data Collecting\n",
    "After the whole process of crawling finished, we can nearly get 6000 documents with the keyword hurricane. And in mongoDB, we can get the metadata of each document. But it is not enough, because there are so many documents that are not papers or they do not have url to PDF, so we need to further process these data.\n",
    "\n",
    "Here is the schema of the paper metadata:\n",
    "\n",
    "```json\n",
    "/* 1 */\n",
    "{\n",
    "    \"_id\" : ObjectId(\"57c6232ddaa39c2add17f1f8\"),\n",
    "    \"document_type\" : \"Conference Paper\",\n",
    "    \"financial_sponsor\" : \"NASA Marshall Space Flight Center; Huntsville, AL United States\",\n",
    "    \"organization_source\" : \"NASA Marshall Space Flight Center; Huntsville, AL United States\",\n",
    "    \"description\" : \"3p; In English\",\n",
    "    \"title\" : \"Validation of Rain Rate Retrievals for the Airborne Hurricane Imaging Radiometer (HIRAD)\",\n",
    "    \"ntrs_full_text\" : [ \n",
    "        \"http://hdl.handle.net/2060/20150022936\"\n",
    "    ],\n",
    "    \"abstract\" : \"The NASA Hurricane and Severe Storm Sentinel (HS3) mission is an aircraft field measurements program using NASA's unmanned Global Hawk aircraft system for remote sensing and in situ observations of Atlantic and Caribbean Sea hurricanes. One of the principal microwave instruments is the Hurricane Imaging Radiometer (HIRAD), which measures surface wind speeds and rain rates. For validation of the HIRAD wind speed measurement in hurricanes, there exists a comprehensive set of comparisons with the Stepped Frequency Microwave Radiometer (SFMR) with in situ GPS dropwindsondes [1]. However, for rain rate measurements, there are only indirect correlations with rain imagery from other HS3 remote sensors (e.g., the dual-frequency Ka- & Ku-band doppler radar, HIWRAP), which is only qualitative in nature. However, this paper presents results from an unplanned rain rate measurement validation opportunity that occurred in 2013, when HIRAD flew over an intense tropical squall line that was simultaneously observed by the Tampa NEXRAD meteorological radar (Fig. 1). During this experiment, Global Hawk flying at an altitude of 18 km made 3 passes over the rapidly propagating thunderstorm, while the TAMPA NEXRAD perform volume scans on a 5-minute interval. Using the well-documented NEXRAD Z-R relationship, 2D images of rain rate (mm/hr) were obtained at two altitudes (3 km & 6 km), which serve as surface truth for the HIRAD rain rate retrievals. A preliminary comparison of HIRAD rain rate retrievals (image) for the first pass and the corresponding closest NEXRAD rain image is presented in Fig. 2 & 3. This paper describes the HIRAD instrument, which 1D synthetic-aperture thinned array radiometer (STAR) developed by NASA Marshall Space Flight Center [2]. The rain rate retrieval algorithm, developed by Amarin et al. [3], is based on the maximum likelihood estimation (MLE) technique, which compares the observed Tb's at the HIRAD operating frequencies of 4, 5, 6 and 6.6 GHz with corresponding theoretical Tb values from a forward radiative transfer model (RTM). The optimum solution is the integrated rain rate that minimizes the difference between RTM and observed values. Because the excess Tb from rain comes from the direct upwelling and the indirect reflected downwelling paths through the atmosphere, there are several assumptions made for the 2D rain distribution in the antenna incident plane (crosstrack to flight direction). The opportunity to knowing 2D rain surface truth from NEXRAD at two different altitudes will enable a comprehensive evaluation to be preformed and reported in this paper.\",\n",
    "    \"subject_category\" : \"METEOROLOGY AND CLIMATOLOGY\",\n",
    "    \"author_and_affiliation\" : [ \n",
    "        [ \n",
    "            \"Jacob, Maria Marta\", \n",
    "            \"National Commission of Space Activities, Buenos Aires, Argentina\"\n",
    "        ], \n",
    "        [ \n",
    "            \"Salemirad, Matin\", \n",
    "            \"University of Central Florida, Orlando, FL, United States\"\n",
    "        ], \n",
    "        [ \n",
    "            \"Jones, W. Linwood\", \n",
    "            \"University of Central Florida, Orlando, FL, United States\"\n",
    "        ], \n",
    "        [ \n",
    "            \"Biswas, Sayak\", \n",
    "            \"NASA Marshall Space Flight Center, Huntsville, AL United States\"\n",
    "        ], \n",
    "        [ \n",
    "            \"Cecil, Daniel\", \n",
    "            \"NASA Marshall Space Flight Center, Huntsville, AL United States\"\n",
    "        ]\n",
    "    ],\n",
    "    \"other_descriptors\" : \"HURRICANE;  RAIN RETRIEVAL\",\n",
    "    \"meeting_information\" : \"IGARSS 2015; 26-31 Jul. 2015; Milan; Italy\",\n",
    "    \"publication_date\" : \"Jul 26, 2015\",\n",
    "    \"meeting_sponsor\" : \"Institute of Electrical and Electronics Engineers; Geoscience and Remote Sensing Society; New York, NY, United States\",\n",
    "    \"patent_number\" : \"MSFC-E-DAA-TN20510\",\n",
    "    \"document_id\" : \"20150022936\",\n",
    "    \"NASA_terms\" : \"IMAGING TECHNIQUES; METEOROLOGICAL RADAR; HURRICANES; MICROWAVE RADIOMETERS; THUNDERSTORMS; RAIN; GROUND TRUTH; RADIATIVE TRANSFER; REMOTE SENSING; TRACKING (POSITION)\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "We can use document_type to filter the documents which are not paper and also we can use ntrs_full_text to get the pdf of this paper, also, the publication_date is very important. We will use it to analyze the topic trend of recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Using __tika__ to get text content of PDF\n",
    "After we store the metadata of crawled papers, we can fetch the url of PDF links from MongoDB, using the url we download the PDF file to temp folder, then parse the file as argument of Open Source Tools tika, it can easily convert PDF to txt content.\n",
    "\n",
    "Apache Tika toolkit detects and extracts metadata and text from over a thousand different file types (such as PPT, XLS, and PDF). And python has a python port library for us to use, https://github.com/chrismattmann/tika-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tika",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-924a270b5904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtika\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtika\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvertPdfToTextByPathAndFilelocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tika"
     ]
    }
   ],
   "source": [
    "import tika\n",
    "from tika import parser\n",
    "import requests\n",
    "\n",
    "def convertPdfToTextByPathAndFilelocation(path):\n",
    "    webPdf = requests.get(path)\n",
    "    # Store as a temporary file\n",
    "    path = \"tmp\"\n",
    "    tmpFile = open(\"tmp\", 'w')\n",
    "    tmpFile.write(webPdf.content)\n",
    "    tmpFile.close()\n",
    "\n",
    "    return pdf2Text(path)\n",
    "\n",
    "tika.TikaClientOnly = True\n",
    "\n",
    "def pdf2Text(filePath):\n",
    "    return parser.from_file(filePath)['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convertPdfToTextByPathAndFilelocation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ecdff6fcfab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20150000264.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mconvertPdfToTextByPathAndFilelocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convertPdfToTextByPathAndFilelocation' is not defined"
     ]
    }
   ],
   "source": [
    "# Example url\n",
    "url = \"https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20150000264.pdf\"\n",
    "print convertPdfToTextByPathAndFilelocation(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Using The Stanford Topic Modeling Toolbox (TMT) to extract Topic of each Paper\n",
    "LDA stands for Latent Dirichlet Allocation, it is a kind of documents topic model. It is also a kind of 3-Layer Bayesian model, including word, topic and document. And for modeling, we can say that each words of one document belongs to one topic with a certain probablity and choose a certain word from this topic. So in this process, document and topic is multinomial distribution, topic and word is multinomial distribution.\n",
    "\n",
    "This process of LDA can be simplified to the following process:\n",
    "1. for each document, choose a topic from topic distribution\n",
    "2. choose a word from the topic's word distribution\n",
    "3. repeat until each word in the document is visited,\n",
    "\n",
    "We can define a document set as D and topic set as T\n",
    "And for each document in D, we can count it as a vector of words, we called it word bag.\n",
    "Also, we need to count all words in D to a set as vocabulary. \n",
    "For a document in D, we can get it's topic distribution and Pti represents the probability of d in number i topics, Pwi represents the probability of i th word in topic t.\n",
    "\n",
    "And the LDA equtaion will be:\n",
    "\n",
    "$P(w|d) = P(w|t) * P(t|d)$\n",
    "\n",
    "Using topic a middle layer and current θd and φt get the probablity of w in d. And with more iterations, we will get Converged result.\n",
    "\n",
    "In this project, we will use The Stanford Topic Modeling Toolbox (TMT) to do the topic modeling on our data. And in order to make the topic more readable and more accurate, we will use the Labeled-LDA feature of TMT.\n",
    "\n",
    "The input is a csv file with the format:\n",
    "\n",
    "| document_id | pre_labels                                                                       | content |\n",
    "|-------------|----------------------------------------------------------------------------------|---------|\n",
    "| 20100032965 | SPECTRAL/ENGINEERING MICROWAVE OCEAN TEMPERATURE BRIGHTNESS TEMPERATURE CYCLONES | .....   |\n",
    "\n",
    "and we also need to use a scala script to define the task and load stopwords.\n",
    "\n",
    "```scala\n",
    "val source = CSVFile(\"%s\") ~> IDColumn(1);\n",
    "\n",
    "import scala.io.Source\n",
    "val listOfLines = Source.fromFile(\"stopwords.txt\").getLines.toList\n",
    "val ll = listOfLines.map( x => x.stripLineEnd )\n",
    "\n",
    "val tokenizer = {\n",
    "  SimpleEnglishTokenizer() ~>            // tokenize on space and punctuation\n",
    "  CaseFolder() ~>                        // lowercase everything\n",
    "  WordsAndNumbersOnlyFilter() ~>         // ignore non-words and non-numbers\n",
    "  MinimumLengthFilter(3)                 // take terms with >=3 characters\n",
    "}\n",
    "\n",
    "val text = {\n",
    "  source ~>                              // read from the source file\n",
    "  Column(3) ~>                           // select column containing text\n",
    "  TokenizeWith(tokenizer) ~>             // tokenize with tokenizer above\n",
    "  TermCounter() ~>                       // collect counts (needed below)\n",
    "  TermMinimumDocumentCountFilter(10) ~>   // filter terms in <4 docs\n",
    "  TermStopListFilter(ll) ~>\n",
    "  TermDynamicStopListFilter(30) ~>       // filter out 30 most common terms\n",
    "  DocumentMinimumLengthFilter(10)         // take only docs with >=5 terms\n",
    "}\n",
    "\n",
    "// define fields from the dataset we are going to slice against\n",
    "val labels = {\n",
    "  source ~>                              // read from the source file\n",
    "  Column(2) ~>                           // take column two, the year\n",
    "  TokenizeWith(WhitespaceTokenizer()) ~> // turns label field into an array\n",
    "  TermCounter() ~>                       // collect label counts\n",
    "  TermMinimumDocumentCountFilter(10)     // filter labels in < 10 docs\n",
    "}\n",
    "\n",
    "val dataset = LabeledLDADataset(text, labels);\n",
    "\n",
    "// define the model parameters\n",
    "val modelParams = LabeledLDAModelParams(dataset);\n",
    "\n",
    "// Name of the output model folder to generate\n",
    "val modelPath = file(\"%s\");\n",
    "\n",
    "// Trains the model, writing to the given output path\n",
    "TrainCVB0LabeledLDA(modelParams, dataset, output = modelPath, maxIterations = 1000);\n",
    "// or could use TrainGibbsLabeledLDA(modelParams, dataset, output = modelPath, maxIterations = 1500);\n",
    "```\n",
    "\n",
    "(We have to use java7 to run this jar. Took us a liitle time to figure out where was going wrong...)\n",
    "\n",
    "After the LLDA is done, we will get 2 useful files: document-topic-distributions.csv and label-index.txt\n",
    "In document-topic-distributions.csv file, we can get the topic probability of each document.\n",
    "\n",
    "|             |    |             |    |             |    |             |    |             |    |             | \n",
    "|-------------|----|-------------|----|-------------|----|-------------|----|-------------|----|-------------| \n",
    "| 20140017430 | 0  | 0.281779471 | 1  | 0.467417845 | 2  | 0.225072019 | 3  | 0.025692527 | 4  | 3.81E-05    | \n",
    "| 20140013335 | 5  | 0.07320749  | 6  | 0.059176645 | 7  | 0.867615866 |    |             |    |             | \n",
    "| 20150000261 | 3  | 2.66E-05    | 6  | 6.92E-06    | 8  | -9.69E-07   | 9  | -6.99E-07   | 10 | 0.999968175 | \n",
    "| 20120002859 | 0  | 3.94E-04    | 2  | 0.579140661 | 11 | 4.42E-05    | 12 | 0.420420941 |    |             | \n",
    "| 20100022203 | 0  | -2.10E-05   | 11 | -1.58E-05   | 12 | 1.000072506 | 13 | -3.57E-05   |    |             | \n",
    "| 20100032965 | 0  | 0.094512137 | 6  | 9.99E-05    | 7  | 0.081092946 | 11 | 3.67E-05    | 14 | 0.824258282 | \n",
    "| 20110006355 | 15 | 1           |    |             |    |             |    |             |    |             | \n",
    "| 20140010541 | 2  | 0.007491234 | 3  | 3.06E-04    | 6  | 0.02016012  | 8  | 0.00551627  | 10 | 0.966525931 | \n",
    "\n",
    "The int is the index of a topic and the float is the probability of this topic in this document\n",
    "\n",
    "And in label-index.txt we can get the list of topics and the index is the index of the topics. We will us python to parse the label-index.txt get the index of each topic and get the topic of each paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'HURRICANES', 1: 'STORM/SURGE', 2: 'STORMS', 3: 'PRECIPITATION', 4: 'EROSION', 5: 'GRAVITY/WAVE', 6: 'CYCLONES', 7: 'TROPICAL/CYCLONES', 8: 'HUMIDITY', 9: 'WIND/SHEAR', 10: 'CONVECTION', 11: 'MICROWAVE', 12: 'SURFACE/WINDS', 13: 'ALTITUDE', 14: 'BRIGHTNESS/TEMPERATURE', 15: '', 16: 'LIDAR', 17: 'LIGHTNING', 18: 'RADAR', 19: 'VORTICITY', 20: 'TURBULENCE', 21: 'WATER/VAPOR', 22: 'CLOUDS', 23: 'SURFACE/PRESSURE', 24: 'CONDENSATION', 25: 'TORNADOES', 26: 'TROPOPAUSE', 27: 'NATURAL/HAZARDS', 28: 'EVAPORATION', 29: 'OSCILLATIONS', 30: 'TYPHOONS', 31: 'AEROSOLS', 32: 'SEA/SURFACE/TEMPERATURE', 33: 'WIND/PROFILES', 34: 'TIDES', 35: 'ATMOSPHERIC/TEMPERATURE', 36: 'HEAT/FLUX', 37: 'OCEAN/CIRCULATION'}\n"
     ]
    }
   ],
   "source": [
    "def parseIndex(filename):\n",
    "    f = open(filename, 'r')\n",
    "    res = {}\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        res[i] = line.strip()\n",
    "        i += 1\n",
    "    return res\n",
    "\n",
    "print parseIndex(\"label-index.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def getTopicOfPaper(dis_csv):\n",
    "    index_dict = parseIndex(\"label-index.txt\")\n",
    "    paper_topic = {}\n",
    "    with open(dis_csv, 'rb') as csvfile:\n",
    "        paper_topic = {}\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            max_index = 0\n",
    "            max_num = float(0)\n",
    "            for i in range(1 , len(row)):\n",
    "                if i%2 == 1:\n",
    "                    continue\n",
    "                if float(row[i]) > float(max_num):\n",
    "                    max_num = float(row[i])\n",
    "                    max_index = i\n",
    "            if max_index - 1 > 0:\n",
    "                paper_topic[row[0]] = index_dict[int(row[max_index - 1])]\n",
    "    return paper_topic\n",
    "\n",
    "topic_dict = getTopicOfPaper(\"document-topic-distributions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Visualization\n",
    "\n",
    "Now we need to find the trend of topics of each year, so we need to do the simple statistical on the data.\n",
    "\n",
    "First we need to get the number of papers of each topic in each year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "def getPapersYear(paperId):\n",
    "    client = MongoClient(host='localhost', port=27017)\n",
    "    papers = client.paper_crawler_all_new.paper\n",
    "    paper = papers.find_one({\"document_id\":str(paperId)})\n",
    "    return paper['publication_date'].split(',')[1].strip()\n",
    "print getPapersYear(20160011133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def getYearTopics(topic_dict):\n",
    "    res = collections.defaultdict(dict)\n",
    "    for k,v in topic_dict.items():\n",
    "        if getPapersYear(k) not in res[v]:\n",
    "            res[v][getPapersYear(k)] = 0\n",
    "        res[v][getPapersYear(k)] += 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we output the data into csv format document, after that we can use these document as raw input, build our frontend viewer to show the topic trend.\n",
    "\n",
    "The data format is shown below:\n",
    "\n",
    "| CountryName             | CountryCode             | 1969 | 1970 | 1971 | 1972 | 1973 | 1974 | 1975 | 1977 | 1978 | 1979 | 1980 | 1981 | 1982 | 1983 | 1984 | 1985 | 1986 | 1987 | 1988 | 1989 | 1990 | 1991 | 1992 | 1993 | 1994 | 1996 | 1999 | 2001 | 2004 | 2005 | 2006 | 2007 | 2008 | 2009 | 2010 | 2011 | 2012 | 2013 | 2014 | 2015 | 2016 |\n",
    "|-------------------------|-------------------------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|\n",
    "| AEROSOLS                | AEROSOLS                | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 1    | 0    | 0    | 0    |\n",
    "| ALTITUDE                | ALTITUDE                | 0    | 1    | 0    | 0    | 0    | 1    | 2    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 5    | 0    | 0    | 0    | 0    | 1    | 1    | 0    | 0    | 0    |\n",
    "| ATMOSPHERIC_TEMPERATURE | ATMOSPHERIC_TEMPERATURE | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    |\n",
    "| BRIGHTNESS_TEMPERATURE  | BRIGHTNESS_TEMPERATURE  | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 2    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1    | 0    | 2    | 1    | 5    | 2    | 0    | 1    | 0    |\n",
    "| CLOUDS                  | CLOUDS                  | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 1    | 2    | 0    | 1    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 2    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    |\n",
    "| CONDENSATION            | CONDENSATION            | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    |\n",
    "| CONVECTION              | CONVECTION              | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 1    | 0    | 1    | 0    | 0    | 1    | 0    | 0    | 1    | 2    | 6    | 2    | 4    | 4    | 1    | 0    | 0    |\n",
    "| EROSION                 | EROSION                 | 0    | 1    | 0    | 0    | 0    | 1    | 1    | 1    | 2    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 2    | 0    | 0    | 0    | 0    |\n",
    "| EVAPORATION             | EVAPORATION             | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 1    | 0    |\n",
    "| GRAVITY_WAVE            | GRAVITY_WAVE            | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 1    | 0    | 2    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    |\n",
    "| HEAT_FLUX               | HEAT_FLUX               | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 1    | 0    | 0    | 1    | 2    | 0    | 0    | 0    | 1    | 0    | 2    | 0    | 0    | 0    |\n",
    "| HUMIDITY                | HUMIDITY                | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 1    | 0    |\n",
    "| HURRICANES              | HURRICANES              | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 2    | 0    | 0    | 0    | 0    | 0    | 0    | 3    | 1    | 1    | 2    | 0    | 1    | 0    | 1    | 0    | 0    |\n",
    "| LIDAR                   | LIDAR                   | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 2    | 1    | 0    | 0    | 0    | 1    | 2    | 0    | 1    | 1    | 0    |\n",
    "| LIGHTNING               | LIGHTNING               | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 1    | 0    | 1    | 0    | 0    | 0    | 0    | 1    | 1    | 0    | 2    | 0    | 3    | 0    | 0    | 1    | 0    | 0    |\n",
    "| NATURAL_HAZARDS         | NATURAL_HAZARDS         | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    |\n",
    "| OCEAN_CIRCULATION       | OCEAN_CIRCULATION       | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 2    | 0    | 0    | 0    |\n",
    "| OSCILLATIONS            | OSCILLATIONS            | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    |\n",
    "| Others                  | Others                  | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 1    | 0    | 2    | 0    | 2    | 0    | 0    | 1    | 0    |\n",
    "| PRECIPITATION           | PRECIPITATION           | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1    | 0    | 0    | 1    | 3    | 2    | 0    | 6    | 0    | 0    | 3    | 0    | 0    | 0    | 1    | 2    | 1    | 1    | 4    | 3    | 9    | 2    | 1    | 0    | 1    |\n",
    "| RADAR                   | RADAR                   | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 2    | 0    | 1    | 0    | 1    | 0    | 0    | 0    |\n",
    "| SEA_SURFACE_TEMPERATURE | SEA_SURFACE_TEMPERATURE | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 2    | 1    | 0    | 3    | 1    | 2    | 2    | 1    | 0    | 0    |\n",
    "| STORM_SURGE             | STORM_SURGE             | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1    | 5    | 1    | 0    | 1    | 0    | 2    | 0    | 1    | 0    | 0    |\n",
    "| STORMS                  | STORMS                  | 0    | 0    | 0    | 1    | 2    | 0    | 4    | 2    | 2    | 0    | 0    | 0    | 1    | 1    | 2    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 2    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 1    | 2    | 0    | 1    | 0    | 0    |\n",
    "| SURFACE_WINDS           | SURFACE_WINDS           | 0    | 1    | 0    | 0    | 1    | 0    | 3    | 0    | 0    | 2    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 2    | 0    | 1    | 0    |\n",
    "| TIDES                   | TIDES                   | 0    | 0    | 2    | 1    | 0    | 0    | 2    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1    | 0    | 0    | 0    |\n",
    "| TORNADOES               | TORNADOES               | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 3    | 0    | 1    | 0    | 0    |\n",
    "| TROPICAL_CYCLONES       | TROPICAL_CYCLONES       | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1    | 0    | 1    | 0    | 0    | 1    | 1    | 4    | 3    | 1    | 0    |\n",
    "| TROPOPAUSE              | TROPOPAUSE              | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 2    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 2    | 3    | 0    |\n",
    "| TURBULENCE              | TURBULENCE              | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    |\n",
    "| TYPHOONS                | TYPHOONS                | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    |\n",
    "| VORTICITY               | VORTICITY               | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 2    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 1    | 1    | 0    | 3    | 1    | 1    | 0    | 0    |\n",
    "| WATER_VAPOR             | WATER_VAPOR             | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    |\n",
    "| WIND_PROFILES           | WIND_PROFILES           | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 2    | 1    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    |\n",
    "| WIND_SHEAR              | WIND_SHEAR              | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 2    | 0    | 0    | 0    | 2    | 0    | 0    | 0    | 0    | 0    | 0    |\n",
    "\n",
    "For the data visulization, we choose to demonstrate our data using the chart, rendered by the html page, to help build with no relief, we utilized the open source Data-Driven Documents(https://d3js.org/) toolkits, it can esily render the page based on your raw datasets.\n",
    "\n",
    "For running the website, we setup it using Node.js with Express.js framework, and deployed it onto AWS EC2, it can now be accessed by: http://ec2-54-211-222-201.compute-1.amazonaws.com/\n",
    "\n",
    "The website shows dynamic chart visulize the data retrieved from the dataset in csv format. Just feel free to play with it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Lesson Learned\n",
    "\n",
    "From this team project, we have grabed a lot of techics and knowledge related to data science, utilizing data analytics tools to analyze the data we crawled and then visulize it, the whole process make us feel excited.\n",
    "\n",
    "At the very begining, we have little experience with gathering data, crawlers, etc. And in this project we practiced heavily, improved from single thread process to multi-thread high-proficiency crawler, we did grow up at data retrieving. We learned the importance of data quality in regards to data processing after, we learned the craw efficiency matters to our project schedule, we keep practicing and finally got the satisfied result.\n",
    "\n",
    "For the data processing stage, we utilized the standford LLDA toolkit to easily analysing raw cralwed PDF content to the result we want, and finally visulize it with beautiful-looking web demo pages."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
