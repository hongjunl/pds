{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLDA TOPIC MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using Scrapy to Get Data from NTRS\n",
    "\n",
    "Scrapy is a very good web crawling framework. We have finished the crawler and have successfully almost 6000 papers from NTRS(NASA Technical Reports Server). Within this framework, we can easily handle request and add dely of each request to make sure that we will not put so much pressure on the server.\n",
    "\n",
    "To use scrapy to crawl papers from NASA, we need to first define items, which is the data structure we will to get from website. Define as following code.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "class PaperCrawlerItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field();\n",
    "    ntrs_full_text = scrapy.Field();\n",
    "    author_and_affiliation = scrapy.Field();\n",
    "    abstract = scrapy.Field();\n",
    "    publication_date = scrapy.Field();\n",
    "    document_id = scrapy.Field();\n",
    "    subject_category = scrapy.Field();\n",
    "    patent_number = scrapy.Field();\n",
    "    document_type = scrapy.Field();\n",
    "    meeting_information = scrapy.Field();\n",
    "    meeting_sponsor = scrapy.Field();\n",
    "    financial_sponsor = scrapy.Field();\n",
    "    organization_source = scrapy.Field();\n",
    "    description = scrapy.Field();\n",
    "    NASA_terms = scrapy.Field();\n",
    "    other_descriptors = scrapy.Field();\n",
    "```\n",
    "\n",
    "First we defined entries for the urls in paper_spider.py:\n",
    "\n",
    "```python\n",
    "def start_requests(self):\n",
    "        for i in range(102231):\n",
    "            yield self.make_requests_from_url(\n",
    "                \"http://ntrs.nasa.gov/search.jsp?Ntx=mode%20matchall&Ntk=All&N=0&No=\" + str(i * 10))\n",
    "```\n",
    "\n",
    "And then, we carefully analyze the html DOM tree to find the values we need and used xpath to extract data we need. This is a very annoying task, because we need to handle every situation and sometimes the elements are not in the same format. \n",
    "\n",
    "After we extracted value from DOM tree and we create a new object item and then we used pipline to store in to MongoDB.\n",
    "\n",
    "Because at first, we assumed that every paper will have a document ID, but we found that, there are many papers without document ID. So, we created out own ID counter, since scrapy is a multi-thread framework, we need add lock to the counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threading' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bac3aabe984b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMongoPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcollection_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'paper'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30000000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bac3aabe984b>\u001b[0m in \u001b[0;36mMongoPipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mcounter_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcounter_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongo_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongo_db\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'threading' is not defined"
     ]
    }
   ],
   "source": [
    "class MongoPipeline(object):\n",
    "\n",
    "    collection_name = 'paper'\n",
    "    global counter\n",
    "    counter = 30000000000\n",
    "\n",
    "    global counter_lock\n",
    "    counter_lock = threading.Lock()\n",
    "\n",
    "    def __init__(self, mongo_uri, mongo_db):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            mongo_uri=crawler.settings.get('MONGO_URI'),\n",
    "            mongo_db=crawler.settings.get('MONGO_DATABASE', 'paper_crawler_all_new')\n",
    "        )\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if dict(item).get('document_id') == \"\":\n",
    "            global counter_lock\n",
    "            counter_lock.acquire()\n",
    "            try:\n",
    "                global counter\n",
    "                item['document_id'] = str(counter)\n",
    "                counter += 1\n",
    "            finally:\n",
    "                counter_lock.release()\n",
    "        self.db[self.collection_name].insert(dict(item))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Mining based on Labeled-LDA\n",
    "\n",
    "After we extract plain text from paper PDF, we get the raw data, next step we will use topic modeling tools to analyze topic keywords of each paper. The tool we use is [Stanford Topic Modeling Toolbox](http://nlp.stanford.edu/software/tmt/tmt-0.4/), it provides functions to train topic models based on Labeled-LDA to create summaries of the text. So to understand how it works, we need to dig into the principle of LDA and Labeled-LDA.\n",
    "\n",
    "\n",
    "## LDA\n",
    "LDA stands for Latent Dirichlet Allocation, is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.\n",
    "\n",
    "As defined in [Wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "\n",
    "\n",
    "In LDA, each document may be viewed as a mixture of various topics. It attempts to guess the topic of a document based on the words that are contained within that document and based on other documents within the same corpus that have similar words. LDA can be thought of as a type of clustering algorithm, where each observation is classified as one of a number of classifications. \n",
    "\n",
    "LDA is a completely unsupervised algorithm that models each document as a mixture of topics. The model generates automatic summaries of topics in terms of a discrete probability distribution over words for each topic, and further infers per-document discrete distributions over topics. Most importantly, LDA makes the explicit assumption that each word is generated from one underlying topic.\n",
    "\n",
    "But LDA is not appropriate for multi-labeled corpora because, as an unsupervised model, it offers no obvious way of incorporating a supervised label set into its learning procedure. In particular, LDA often learns some topics that are hard to interpret, and the model provides no tools for tuning the generated topics to suit an end-use application, even when time and resources exist to provide some document labels. So we use Labeled-LDA, one supervised way to correct training model.\n",
    "\n",
    "## Labeled-LDA \n",
    "\n",
    "Labeled LDA (L-LDA), a generative model for multiplying labeled corpora that marries the multi-label supervision common to modern text datasets with the word-assignment ambiguity resolution of the LDA family of models. In contrast to standard LDA and its existing supervised variants, our model associates each label with one topic in direct correspondence.\n",
    "\n",
    "Like Latent Dirichlet Allocation, Labeled LDA models each document as a mixture of underlying topics and generates each word from one topic. Unlike LDA, L-LDA incorporates supervision by simply constraining the topic model to use only those topics that correspond to a documentâ€™s (observed) label set.\n",
    "\n",
    "To implement L-LDA, we use Stanford TMT toolbox to simply perform training topic model on the given dataset, we store each paper's raw text into csv format, use this as input dataset, then run TMT Toolbox we downloaded.\n",
    "\n",
    "## TMT Toolbox\n",
    "The Stanford Topic Modeling Toolbox (TMT) brings topic modeling tools to social scientists and others who wish to perform analysis on datasets that have a substantial textual component. It has easily usable Labeled-LDA training model function, to use it we need to specify a LabeledLDA dataset, and tell the toolbox where the text comes from as well as where the labels come from. \n",
    "\n",
    "Below is our config script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val source = CSVFile(\"%s\") ~> IDColumn(1);\n",
    "\n",
    "import scala.io.Source\n",
    "val listOfLines = Source.fromFile(\"stopwords.txt\").getLines.toList\n",
    "val ll = listOfLines.map( x => x.stripLineEnd )\n",
    "\n",
    "val tokenizer = {\n",
    "  SimpleEnglishTokenizer() ~>            // tokenize on space and punctuation\n",
    "  CaseFolder() ~>                        // lowercase everything\n",
    "  WordsAndNumbersOnlyFilter() ~>         // ignore non-words and non-numbers\n",
    "  MinimumLengthFilter(3)                 // take terms with >=3 characters\n",
    "}\n",
    "\n",
    "val text = {\n",
    "  source ~>                              // read from the source file\n",
    "  Column(3) ~>                           // select column containing text\n",
    "  TokenizeWith(tokenizer) ~>             // tokenize with tokenizer above\n",
    "  TermCounter() ~>                       // collect counts (needed below)\n",
    "  TermMinimumDocumentCountFilter(10) ~>   // filter terms in <4 docs\n",
    "  TermStopListFilter(ll) ~>\n",
    "  TermDynamicStopListFilter(30) ~>       // filter out 30 most common terms\n",
    "  DocumentMinimumLengthFilter(10)         // take only docs with >=5 terms\n",
    "}\n",
    "\n",
    "// define fields from the dataset we are going to slice against\n",
    "val labels = {\n",
    "  source ~>                              // read from the source file\n",
    "  Column(2) ~>                           // take column two, the year\n",
    "  TokenizeWith(WhitespaceTokenizer()) ~> // turns label field into an array\n",
    "  TermCounter() ~>                       // collect label counts\n",
    "  TermMinimumDocumentCountFilter(10)     // filter labels in < 10 docs\n",
    "}\n",
    "\n",
    "val dataset = LabeledLDADataset(text, labels);\n",
    "\n",
    "// define the model parameters\n",
    "val modelParams = LabeledLDAModelParams(dataset);\n",
    "\n",
    "// Name of the output model folder to generate\n",
    "val modelPath = file(\"%s\");\n",
    "\n",
    "// Trains the model, writing to the given output path\n",
    "TrainCVB0LabeledLDA(modelParams, dataset, output = modelPath, maxIterations = 1000);\n",
    "// or could use TrainGibbsLabeledLDA(modelParams, dataset, output = modelPath, maxIterations = 1500);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the script, using bash command ```jre1.7.0_80/bin/java -jar tmt-0.4.0.jar script``` to run LLDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
